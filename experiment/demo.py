from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import sys
import pickle

import numpy as np
from six.moves import xrange  # python2/3 compatible 
import tensorflow as tf
import string
import scipy 
import scipy.sparse as sparse
import os

# import code of this project
sys.path.insert(0, '../util/')
from util import config_to_name
sys.path.insert(0, '../model/')
from embedding import fit_emb
from embedding import evaluate_emb
from embedding import dense_array_feeder
from embedding import sparse_array_feeder
from random_data import rand_data

def embedding_experiment(config, dataset):
    np.random.seed(seed=27)

    ## Step 1: load data
    print('Generating a dataset ...')

    data = rand_data() # the training/test dataset generated by rand_data has two fields, but only 'scores' are needed here

    trainset = data['trainset']['scores']
    testset = data['testset']['scores']

    """
        trainset: scores: a sparse matrix, each ij entry is the rating of movie j given by person i, or the count of item j in basket i
        testset:  [same structure as trainset]
    """

    # one can always redefine zie.generate_batch(reviews, rind) to use other format of trainset and testset 

    print('The training set has %d rows and %d columns, and the test set has %d rows' % 
                             (trainset.shape[0], trainset.shape[1], testset.shape[0]))
    



    # batch_feeder is a function, which will be executed as batch_feeder(trainset[i])
    # its output will be fed into tf place holders
    batch_feeder = sparse_array_feeder

    # fit an emb model
    print('Training set has size: ', trainset.shape)
    emb_model, logg = fit_emb(trainset, batch_feeder, config)
    print('Training done!')

    print('Test set has size: ', testset.shape)
    test_llh = evaluate_emb(testset, batch_feeder, emb_model, config)
    print('Testing done!')

    # Save result 
    print('Check result...')
    emb_vec = emb_model['alpha']
    print('Embedding matrix has shape ', emb_vec.shape)
    # Save wherever you want 
 
    print('Done!')

if __name__ == '__main__':

    dataset = 'random'
    dist = 'poisson'
    max_iter = 500
    nprint = 100

    config = dict(
                  # the dimensionality of the embedding vectors  
                  K=50,              
                  # the embedding distribution  'poisson' or 'binomial' (N=3)
                  dist=dist,        
                  # ratio of negative samples. if there are N0 zeros in one row, only sample (0.1 * N0) from these zero,  
                  # it is equivalent to downweight zero-targets with weight 0.1 
                  neg_ratio=0.1,    
                  # number of optimization iterations
                  max_iter=max_iter, 
                  # number of iterations to print objective, training log-likelihood, and validation log-likelihood, and debug values
                  nprint=nprint, 
                  # weight for regularization terms of embedding vectors
                  ar_sigma2=1, 
                  # uncomment the following line to use the base model
                  #model='base', 
                  # uncomment the following line to use context selection. Only the prior 'fixed_bern' works for now 
                  model='context_select', prior='fixed_bern', nsample=30, hidden_size=[30, 15], histogram_size=40, nsample_test=1000, selsize=10,
                  ) 

    print('The configuration is: ')
    print(config)

    embedding_experiment(config, dataset)
    


